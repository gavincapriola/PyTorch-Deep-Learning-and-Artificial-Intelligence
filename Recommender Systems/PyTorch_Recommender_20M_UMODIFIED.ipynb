{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVdFJm9T36BczgBQw8svt4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gavincapriola/PyTorch-Deep-Learning-and-Artificial-Intelligence/blob/main/Recommender%20Systems/PyTorch_Recommender_20M_UMODIFIED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sklDiNCRTBC1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data is from: https://grouplens.org/datasets/movielens/\n",
        "!wget -nc https://files.grouplens.org/datasets/movielens/ml-20m.zip"
      ],
      "metadata": {
        "id": "xSDRhVp6YM90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -n ml-20m.zip"
      ],
      "metadata": {
        "id": "ck3KuJ57YU_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "jaewJ98eYfy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('ml-20m/ratings.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Q9p4dWuvYg3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can't trust the userId and movieId to be numbered 0...N-1\n",
        "# Let's just set our own ids\n",
        "\n",
        "# current_user_id = 0\n",
        "# custom_user_map = {} # old user id > new user id\n",
        "# def map_user_id(row):\n",
        "#     global current_user_id, custom_user_map\n",
        "#     old_user_id = row['userId']\n",
        "#     if old_user_id not in custom_user_map:\n",
        "#         custom_user_map[old_user_id] = current_user_id\n",
        "#         current_user_id += 1\n",
        "#     return custom_user_map[old_user_id]\n",
        "\n",
        "# df['new_user_id'] = df.apply(map_user_id, axis=1)\n",
        "\n",
        "df.userId = pd.Categorical(df.userId)\n",
        "df['new_user_id'] = df.userId.cat.codes"
      ],
      "metadata": {
        "id": "X4XLtA3yYl85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now do the same for movieId\n",
        "\n",
        "# current_movie_id = 0\n",
        "# custom_movie_map = {} # old movie id > new movie id\n",
        "# def map_movie_id(row):\n",
        "#     global current_movie_id, custom_movie_map\n",
        "#     old_movie_id = row['movieId']\n",
        "#     if old_movie_id not in custom_movie_map:\n",
        "#         custom_movie_map[old_movie_id] = current_movie_id\n",
        "#         current_movie_id += 1\n",
        "#     return custom_movie_map[old_movie_id]\n",
        "\n",
        "# df['new_movie_id'] = df.apply(map_movie_id, axis=1)\n",
        "\n",
        "df.movieId = pd.Categorical(df.movieId)\n",
        "df['new_movie_id'] = df.movieId.cat.codes"
      ],
      "metadata": {
        "id": "yeGzMDZ9Z_lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user IDs, and ratings as separate arrays\n",
        "user_ids = df['new_user_id'].values\n",
        "movie_ids = df['new_movie_id'].values\n",
        "ratings = df['rating'].values - 2.5"
      ],
      "metadata": {
        "id": "LiSH7undaCZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of users and number of movies\n",
        "N = len(set(user_ids))\n",
        "M = len(set(movie_ids))\n",
        "\n",
        "# Set embedding dimension\n",
        "D = 10"
      ],
      "metadata": {
        "id": "4oVUisfBaEy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the neural network\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, n_users, n_items, embed_dim, n_hidden=1024):\n",
        "        super(Model, self).__init__()\n",
        "        self.N = n_users\n",
        "        self.M = n_items\n",
        "        self.D = embed_dim\n",
        "        \n",
        "        self.u_emb = nn.Embedding(self.N, self.D)\n",
        "        self.m_emb = nn.Embedding(self.M, self.D)\n",
        "        self.fc1 = nn.Linear(2*self.D, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, 1)\n",
        "        \n",
        "    def forward(self, u, m):\n",
        "        u = self.u_emb(u) # output is (num_samples, D)\n",
        "        m = self.m_emb(m) # output is (num_samples, D)\n",
        "        \n",
        "        # merge\n",
        "        out = torch.cat((u, m), 1) # output is (num_samples, 2D)\n",
        "        \n",
        "        # ANN\n",
        "        out = self.fc1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "USiEhHYFaOAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "cEXHBGbda4O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(N, M, D)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "37i_fJPOa72A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "Gde6O4ZXbS_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the data in corresponding order\n",
        "user_ids, movie_ids, ratings = shuffle(user_ids, movie_ids, ratings)"
      ],
      "metadata": {
        "id": "M-3QuOjtbUIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to tensors\n",
        "user_ids_t = torch.from_numpy(user_ids).long()\n",
        "movie_ids_t = torch.from_numpy(movie_ids).long()\n",
        "ratings_t = torch.from_numpy(ratings)"
      ],
      "metadata": {
        "id": "rcfe6o1TbVZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make dataset\n",
        "Ntrain = int(0.8 * len(ratings))\n",
        "train_dataset = torch.utils.data.TensorDataset(user_ids_t[:Ntrain], movie_ids_t[:Ntrain], ratings_t[:Ntrain])\n",
        "test_dataset = torch.utils.data.TensorDataset(user_ids_t[Ntrain:], movie_ids_t[Ntrain:], ratings_t[Ntrain:])"
      ],
      "metadata": {
        "id": "ehWB6ukibWZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loader\n",
        "batch_size = 512\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "ugFG3yZUb4zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to encapsulate the training loop\n",
        "def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):\n",
        "    train_losses = np.zeros(epochs)\n",
        "    test_losses = np.zeros(epochs)\n",
        "    \n",
        "    for it in range(epochs):\n",
        "        t0 = datetime.datetime.now()\n",
        "        train_loss = []\n",
        "        for users, movies, targets in train_loader:\n",
        "            targets = targets.view(-1, 1).float()\n",
        "            \n",
        "            # move data to GPU\n",
        "            users, movies, targets = users.to(device), movies.to(device), targets.to(device)\n",
        "            \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(users, movies)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            # Backward and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss.append(loss.item())\n",
        "        \n",
        "        # Get train loss and test loss\n",
        "        train_loss = np.mean(train_loss) # a little misleading\n",
        "        \n",
        "        test_loss = []\n",
        "        for users, movies, targets in test_loader:\n",
        "            users, movies, targets = users.to(device), movies.to(device), targets.to(device)\n",
        "            targets = targets.view(-1, 1).float()\n",
        "            outputs = model(users, movies)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss.append(loss.item())\n",
        "        test_loss = np.mean(test_loss)\n",
        "        \n",
        "        # Save losses\n",
        "        train_losses[it] = train_loss\n",
        "        test_losses[it] = test_loss\n",
        "        \n",
        "        dt = datetime.datetime.now() - t0\n",
        "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Duration: {dt}')\n",
        "        \n",
        "        return train_losses, test_losses"
      ],
      "metadata": {
        "id": "XeQ37kpWb5T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# profile this using\n",
        "%prun train_losses, test_losses = batch_gd(model, criterion, optimizer, train_loader, test_loader, 25)"
      ],
      "metadata": {
        "id": "5zlrLoGXdLO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YsR1vTUAdVPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}